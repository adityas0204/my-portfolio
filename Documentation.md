## Logging Documentation

I will be documenting my process of creating a custom logging service to measure metrics related to my site, which I have not decided at the moment of writing this.

I spent a day creating my server. I used express and mongoose. I had a few bugs here and there but I was able to figure them out fairly smoothly. I think most of the work I will be doing on the server will be figuring out what my models should look like, since I haven't decided what I want to really log yet. 

An issue that i've encountered is that I can't use firebase to host my server. Firebase hosting only hosts static sites. Firebase has functions as well, but I won't be able to use MongoDB. Maybe I can use Amazon AWS instead if they have a student discount or free credits, but I'll look into this later.

I've also created a connection from my frontend to my server. Since I'm using REST API's, I use axios to send put requests. Currently, I'm calling them pings, and I've made it so that a ping is sent when a person visits my site by using React's `useEffect` hook in my app . This hook will come in handy again if I want to send pings on an time interval basis. 

While doing this I also found a silly bug; I used `PUT` instead of `POST` in my axios method call, causing my backend to not recgonize the HTTP request and retuning 404. 

Now that I've figured out how to connect my frontend to my backend, I need to etermine what I want to actually log. So far, I've got a few ideas:
- Unique visitors
- Time spent on my site
- Which type of device they connect from
- How many times they hover links in the home page 
  - They have a cool animation that I'm proud of ykwim
- Maybe how far they scroll as well

These requirements will change depending on what new things I think of or encounter as I research this topic.

I have some idea for how I should create my models:

```json
{ 
    ip: string
    visit: {
        date: string
        visitLength: int
        homeUrlHoverCount: int
        scrollDistance: string
    }
}
```

A lot of this information can not be determined from one individual ping being sent. For example, the `visitLength` would have to be two pings, one for when they launch the site and another for when they close the page or maybe swap windows. I'm thinking that maybe every visit has an ID that gets sent from the server to the client, which the client can use in its future requests. Instead of `scrollDistance` being a string it could be a set for which parts of the page they visit. A set would prevent duplicates and be easier to manage. The client could also have a copy of the set and if the client visits a new section of the site then local set copy updates, after which the client can make a request to update the server's copy. 

The `homeUrlHoverCount` could be counting locally, and then when the connection is terminated it can send the updated copy in the same request as the `visitLength` one. Infact, I think we can store all this data locally, and then we can send a request to the server when the webpage is being closed. I'm not sure how we would manage a person who has the site open in the background. Maybe we send a request to the server when they switch tabs and close the tab. If someone is idling, then maybe we send a request every five minutes. We need to ensure that we only make as many pings as we need to, to stay efficient and potentially cost efficient.

After presenting my plan to an LLM and hearing its input, I've landed on a good scheme. Also, we will be storing hashes of ip addresses for security reasons, but I don't think anyone will be hacking into my db. This will just be a little more experience. 

New schema:

```json
{
    id: int (generated by MongoDB)
    hashedIp: string,
    device: string,
    timings: {
        start: date,
        lastPing: data,
        durationSec: int
    },
    interactions: {
        hoverCount: int,
        scrollMilestones: [string]
    }, 
}
```

I started coding this, adding the route for `PUT` and then I saw a method called `findByIdAndUpdate`. It's an atomic method where I can use MongoDB methods like `$inc` and `$set`. The idea now is to send all non atomic data in the first `POST`  request thats made, and then send `PUT` methods with codes like `HOVER` or `PING` in the request body to indicate that we need to increment the `hoverCount`. This will also eliminate race conditions as I won't go back and forth between MongoDB two times. 

I got the hover pings to work. I had to use `ref`s so that the values wouldn't be lost over React renders. I also did not want to send an update every time the user hovers so I used two `ref`s; one `ref` to start a timer, and another to act as a buffer and count the number of hovers performed in that time. 

For the scroll tracking, I used a custom hook alongside an `IntersectionObserver`. This is a really cool built in API that lets me easily check if my target, as in the component I want to check, is present on the screen. I used a `ref` to pass the DOM element that I wanted to target into the hook by attaching it to the div:

```js
<footer ref={scrollRef} ...>
    ...
</footer>
```

I attached the hook onto every component I wanted to track.

Then, I ran into an issue. If I launched the site, then the `createPing` wouldn't be processed in time for the home screen to send an `updateLog` for the screen tracking. To manage this race condition I set the promise inside of `createPing` to a variable declared outside of the function. `updateLog` could then check if that promise existed, and if it did then it would `await` it, pausing the execution of `updateLog` until the promise had beed fulfilled. This can be seen in `/client/src/services/ping`.

Setting up the `HEARTBEAT` pings was very easy. I used a `setInterval` callback function that would simply call `updateLog` every 30 seconds. However, I ran into a weird Mongoose problem where my query to update `timings.durationSec` in my backend wasn't being allowed due to Mongoose getting confused. I found a Stack Overflow thread that said the solution was to access MongoDB's collections through the Mongoose objects to bypass Mongoose's security checks. That ended up being the fix.

## Deployment Documentation

After doing lots of research, I have decided to use AWS EC2. It is the cheapest option in the long run, but it requires more setup and maintenance. I think this will be a great learning experience for me since I have always sed hosting solutions that manage everything for me.

Firstly, I looked online for tutorials. I found [one video](https://www.youtube.com/watch?v=nQdyiK7-VlQ) and [one site](https://www.digitalocean.com/community/tutorials/how-to-set-up-a-node-js-application-for-production-on-ubuntu-20-04). I used two sources to have a mix and followed both simultaneously to see how things are done differently. 

Then, I began the EC2 setup. First, I started an EC2 instance with Ubuntu as the OS. I also downloaded a `.pem` file to ssh into the machine. I ssh'ed into the machine through my WSL terminal and first ran `upgrade` and `update` commands. To my knowledge, I will have to do this semi-frequently to make sure that I am up to date on security updates. 

I then installed `Node` since my app was a Node app. I connected the machine to my github through the ssh key, and pulled my project into the machine. Once my repo was on the machine, I realized that I couldn't just run my app since I hadn't setup the `NODE_ENV` variables in `package.json`. I installed `cross-env` so that it wouldn't be an issue on Windows vs. Linux and started writing my deploy and start commands. My deploy command initially was made for Windows and wouldn't work on Linux machines (due to how I was moving `dist` folder). To fix this I installed another module `shx` that let the command work for Windows and Linux. I deployed the app, and went to the public IP of my machine, but nothing was shown there. The tutorial said to open the port through AWS that my app was running on, and then when I went to that port I could see my app running. No data was being logged in MongoDB since my application was not connecting to Atlas. To solve the problem I authorized connections from anywhere for Atlas. 

This was all very exciting but there was still much work to go. Next, I install PM2. This software makes sure that my application is constantly running on the machine, even if I close the terminal or my connection. It was very easy setting up PM2.

Next, it was time to setup Nginx. This is a webserver that works by creating a reverse proxy between port 80 and port 3001, where my server is running. The setup was pretty simple, I had to edit `nginx/sites-available/my-portfolio`. I added my server name, and some proxy setup. 

Then, I changed my DNS IP on porkbun (my DNS registrar) to my EC2 instance public IP. So now, my `http` site works, but to make my `https` site to work I needed to add certificates. 

For certificates, I used Certbot. Initially, I tried to use `soniaditya.com` and `www.soniaditya.com`, but it couldnt find my site on the DNS despite me having changed it in my DNS registrar. I did some research and determiend that the problem was due to DNS propogation, which could take upto 48H. 

I checked back the next day, and my DNS was working. Certbot worked for `soniaditya.com`, but not for `www.soniaditya.com`. This was due to me not putting the www site as a canonical name, and after doing that all routing would go to `www.soniaditya`. With the SSL certificates working now, my site would appear as `https`.

Finally, I installed and setup Brotli for compression for bettwe compression.

The site is up and running now, but there is still much work to do on the instance such as load management and monitoring. But all of that is for later. 

## Linting

I want to stanardize some of my coding norms through linting. One of my folders is CommonJS and the other is Esmodules, so they both have their own `eslint.config` files. I looked at the ESLint documentation and picked out some basic rules:

```js
rules: {
  'no-unused-vars': ['error', { varsIgnorePattern: '^[A-Z_]' }],
  'indent': ['error', 2],
  'camelcase': 'warn',
  'eqeqeq': 'warn',
  'quotes': ['error', 'single'],
  'semi': ['error', 'always'],
}
```

As time goes on, I think I will find and add more rules but for now these are good enough. I added two scripts, one for basic linting and another for fixing the problems:

```json
"lint": "eslint .",
"lint:fix": "eslint --ext js,jsx --fix"
```

## Miscellanous Issues

An issue I have noticed is that when I start my backend in dev mode, it connects to my production database. Furthermore, sometimes I don't want to send pings altogether. I can make a test database, and add the key to my `.env`. Then I can alternate the use of the keys in my `util` file. 

Actually, I don't wanna make a separate DB on Atlas, and if I use the same one then I can't make a another free cluster. To resolve this issue I'll just make a new collection in the same cluster. I don't expect a lot of traffic anyways, so this should be fine.